{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_j_Gu7s3wTO"
   },
   "source": [
    "# Let's build and initialise a MedCAT model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4bQfWfXlKWJ"
   },
   "source": [
    "### First we need to install MedCAT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OlMQ2iQrlG69",
    "outputId": "f0ab90b4-b587-411d-fb0b-25662d293d6b"
   },
   "outputs": [],
   "source": [
    "# Install MedCAT\n",
    "#! pip install medcat==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the scispacy model\n",
    "#! python -m spacy download en_core_web_md\n",
    "#try:\n",
    "#    from medcat.cat import CAT\n",
    "#except:\n",
    "#    print(\"WARNING: Runtime will restart automatically and please run other cells thereafter.\")\n",
    "#    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWScf8BW0BpY"
   },
   "source": [
    "**Restart the runtime if on colab, sometimes necessary after installing models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "KByaUPYNk7gk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from medcat.vocab import Vocab\n",
    "from medcat.cdb import CDB\n",
    "from medcat.config import Config\n",
    "from medcat.cdb_maker import CDBMaker\n",
    "from medcat.cat import CAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "kKgZTiZxk7gp"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = r'C:\\Users\\aizaa\\OneDrive\\Documents\\2022_NHSX_Internship\\Codes\\MedCATtutorials\\notebooks\\introductory\\data'\n",
    "#! DATA_DIR=\"./introductory/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IRLcv4dGIEbS",
    "outputId": "02a09376-e417-49be-eda5-06dbac5e8f3c"
   },
   "outputs": [],
   "source": [
    "# Load files if in google colab, otherwise skip this step\n",
    "!wget -N https://raw.githubusercontent.com/CogStack/MedCATtutorials/main/notebooks/introductory/data/cdb_simple.csv -P $DATA_DIR\n",
    "!wget -N https://raw.githubusercontent.com/CogStack/MedCATtutorials/main/notebooks/introductory/data/cdb_advanced.csv -P $DATA_DIR\n",
    "!wget -N https://raw.githubusercontent.com/CogStack/MedCATtutorials/main/notebooks/introductory/data/vocab_data.txt -P $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kj24ZU79D-xE"
   },
   "source": [
    "# MedCAT Components\n",
    "The medcat model requires 3 model components to run.\n",
    "1. Vocab\n",
    "2. CDB\n",
    "3. Config (cdb configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9POZ_dwsk7gu"
   },
   "source": [
    "## Building a Vocabulary\n",
    "\n",
    "The first of the two required models when running MedCAT is a Vocabulary model (Vocab). The model is used for two things: (1) Spell checking; and (2) Word Embedding. \n",
    "\n",
    "The Vocab is very simple and you can easily build it from a file that is structured as below:\n",
    "```\n",
    "<token>\\t<word_count>\\t<vector_embedding_separated_by_spaces>\n",
    "```\n",
    "`token` - Usually a word or subword if you are using Byte Pair Encoding or something similar.\n",
    "\n",
    "`word_count` - The count for this word in your dataset or in any large dataset (wikipedia also works nicely).\n",
    "\n",
    "`vector_embedding_separated_by_spaces` - precalculated vector embedding, can be from Word2Vec or BERT.\n",
    "\n",
    "---\n",
    "An example with 3-dimension embedding would be:\n",
    "```\n",
    "house\t34444\t 0.3232 0.123213 1.231231\n",
    "dog\t14444\t0.76762 0.76767 1.45454\n",
    ".\n",
    ".\n",
    ".\n",
    "```\n",
    "The file is basically a TSV, but should not have any heading. \n",
    "\n",
    "---\n",
    "\n",
    "**NOTE**: If spelling is important for your use-case, take care that there are no misspelt words in the Vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "OgMSGHyhk7gv"
   },
   "outputs": [],
   "source": [
    "# Let's have a look at an example, I've created a small vocabulary with only 2 words (the ones from above)\n",
    "# Let's try to create a vocabulary from this two words.\n",
    "\n",
    "vocab_path = r'C:\\Users\\aizaa\\OneDrive\\Documents\\2022_NHSX_Internship\\Codes\\MedCATtutorials\\notebooks\\introductory\\data\\vocab_data.txt'\n",
    "\n",
    "vocab = Vocab()\n",
    "vocab.add_words(vocab_path, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPl6ghXUk7gy"
   },
   "source": [
    "**And that is everything, with this we have built our vocab and no futher training is necessary.**\n",
    "\n",
    "---\n",
    "\n",
    "A couple of useful functions of the vocab are presented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HeVMPs5Zk7gy",
    "outputId": "3f5bc7da-1958-402c-da80-3ce3fba9d860"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['house', 'dog'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see the words in the vocab\n",
    "vocab.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TofkHoHo0XKU",
    "outputId": "14b5257e-63d4-4eb4-f743-0afea32ffa51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'house': {'vec': array([0.3232  , 0.123213, 1.231231]),\n",
       "  'cnt': 34444,\n",
       "  'ind': 0},\n",
       " 'dog': {'vec': array([0.76762, 0.76767, 1.45454]), 'cnt': 14444, 'ind': 1}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SclVqlDgk7g2",
    "outputId": "63cb7ed9-0f50-402c-f6e3-2c49edaa4354"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['house', 'dog', 'test'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to add words manually (one by one) use:\n",
    "vocab.add_word(\"test\", cnt=31, vec=np.array([1.42, 1.44, 1.55]), replace=True)\n",
    "vocab.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnCgpKrEk7g5",
    "outputId": "b5ca8d86-fa73-48d8-adbf-1fc86f1d8ba7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3232  , 0.123213, 1.231231])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get a vector of word use:\n",
    "vocab.vec(\"house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bO4IEvrJk7g8",
    "outputId": "62e12cad-e80e-4083-a11a-ec53b8ec3a19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34444"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or to get the count\n",
    "vocab['house']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLg63Z9Yk7g-",
    "outputId": "c63b0160-b7ea-4fdf-98a9-96a31ab744be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check if a word is in the vocab:\n",
    "\"house\" in vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xG3FCinSl_Sq"
   },
   "source": [
    "### Before we save the vocab model, we need to create the unigram table for negative sampling for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wdC8eennmSM4"
   },
   "outputs": [],
   "source": [
    "# This is necessary after each change of the vocabulary (when we add new words)\n",
    "vocab.make_unigram_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6itJcEXk7hA"
   },
   "source": [
    "### Save the Vocab model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "RgRtW7eqk7hB"
   },
   "outputs": [],
   "source": [
    "vocab.save(DATA_DIR + \"/vocab.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YBbwcNUk7hD"
   },
   "source": [
    "### Load the Vocab model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "o_tOsE6ak7hE"
   },
   "outputs": [],
   "source": [
    "vocab = Vocab.load(DATA_DIR + \"vocab.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptRmHln9k7hG"
   },
   "source": [
    "## Building the Concept Database (CDB)\n",
    "\n",
    "The second model we are going to need when using MedCAT is the Concept Database (CDB). This database holds a list of all concepts that we would like to detect and link to. For a lot of medical use-cases we would use giant databases like the UMLS or SNOMED CT. However, MedCAT can be used with any database no matter how big/small it is. \n",
    "\n",
    "To prepare the CDB we start off with a CSV with the following structure:\n",
    "```\n",
    "cui,name\n",
    "1,kidney failure\n",
    "7,CoVid 2\n",
    "7,coronavirus\n",
    "```\n",
    "This is the most basic version of the CSV file, it has only:\n",
    "\n",
    "`cui` - The concept unique identifier, this is simply an `ID` in your database.\n",
    "\n",
    "`name` - String/Name of that concept. It is important to write all possible names and abbreviations for a concept of interest.\n",
    "\n",
    "If you have a concept that can be recognised through multiple different names (like the one above with cui=7), you can simply add multiple rows with the same concept ID and MedCAT will merge that during the build phase.\n",
    "\n",
    "## The Full CSV Specification\n",
    "```\n",
    "cui,name,ontologies,name_status,type_ids,description\n",
    "1,Kidney Failure,SNOMED,P,T047,kidneys stop working\n",
    ".\n",
    ".\n",
    ".\n",
    "```\n",
    "The rest of the fields are optional, each can be included or left out in your CSV:\n",
    "\n",
    "`ontologies` - Source ontology, e.g. HPO, SNOMED, HPC,...\n",
    "\n",
    "`name_status` - Term type e.g. P - Primary Name. Primary names are important and I would always recommend to add this fields when creating your CDB. This will help distinguish between synonyms.\n",
    "\n",
    "`type_ids` - Type Ids are the broad category in which a concept may fall under. This is used to rapidly filter for concepts which fall under a specific category. In UMLS this could be the Semantic type identifier - e.g. T047 (taken from UMLS). A list of all semantic types can be found [here](https://metamap.nlm.nih.gov/Docs/SemanticTypes_2018AB.txt).\n",
    "In SNOMED one could use the Semantic tags. E.g (Disease). A list of all Snomed semantic tags can be found [here](https://confluence.ihtsdotools.org/display/DOCGLOSS/semantic+tag).\n",
    "\n",
    "\n",
    "`description` - Description of this concept\n",
    "\n",
    "***Note***: If one concept has multiple names, you can also separate the different names by a \"|\" - pipe - symbol "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "y5IR-xFO4oph"
   },
   "outputs": [],
   "source": [
    "cdb_simple = pd.read_csv(DATA_DIR + '/cdb_simple.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "tHr26s3s4vJN",
    "outputId": "60bfd389-347a-44d4-bbe1-ab181eadda96"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cui</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kidney failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>CoVid 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cui            name\n",
       "0    1  kidney failure\n",
       "1    7         CoVid 2\n",
       "2    7     coronavirus"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdb_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rasu5PajojYZ"
   },
   "source": [
    "Let's try building our own concept databse from a simple CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "y75WVeca08z3"
   },
   "outputs": [],
   "source": [
    "# First initialise the default configuration\n",
    "config = Config()\n",
    "config.general['spacy_model'] = 'en_core_web_md'\n",
    "maker = CDBMaker(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "op5-LDOfk7hH",
    "outputId": "253f5a44-5a1c-4d5b-8832-c7823b988092"
   },
   "outputs": [],
   "source": [
    "# Create an array containing CSV files that will be used to build our CDB\n",
    "csv_path = [ DATA_DIR + '/cdb_advanced.csv', DATA_DIR + '/cdb_simple.csv',]\n",
    "\n",
    "# Create your CDB\n",
    "cdb = maker.prepare_csvs(csv_path, full_build=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08agsFBnk7hQ"
   },
   "source": [
    "**That is all, nothing else is necessary to build the CDB**\n",
    "\n",
    "---\n",
    "\n",
    "Some useful functions of the cdb are below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwlOBilek7hJ",
    "outputId": "664c799b-4c5a-46e2-987a-2565b757936e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kidney~failure': ['1'], 'failure~of~kidneys': ['1'], 'failure~of~kidney': ['1'], 'kf': ['1'], 'k~.~failure': ['1'], 'covid~2': ['7'], 'coronavirus': ['7']}\n"
     ]
    }
   ],
   "source": [
    "# To display all names and cui in the db\n",
    "print(cdb.name2cuis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uD3AYqqkk7hQ",
    "outputId": "9cf3660a-82ff-4844-c038-2cd02c02f1c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'kidney~failure', 'kf', 'failure~of~kidney', 'k~.~failure', 'failure~of~kidneys'}, '7': {'coronavirus', 'covid~2'}}\n"
     ]
    }
   ],
   "source": [
    "# To display all unique cuis and corresponding names in the db \n",
    "print(cdb.cui2names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWTRmz_duRT2",
    "outputId": "150f6d69-3a07-4aa9-a963-62c1906ba15a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 'Kidney Failure'}\n"
     ]
    }
   ],
   "source": [
    "# To display cui to preferred name\n",
    "print(cdb.cui2preferred_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j18tyQ2Kk7hV",
    "outputId": "745889c2-e8b5-47d5-f985-035432732128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'T047'}, '7': set()}\n"
     ]
    }
   ],
   "source": [
    "# We have a link from cui to type ids\n",
    "print(cdb.cui2type_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lpx7zGvwk7ha"
   },
   "source": [
    "### Save the Concept Database model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "fC01maOzk7ha"
   },
   "outputs": [],
   "source": [
    "cdb.save(DATA_DIR + \"/cdb.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97uiDwvAk7hc"
   },
   "source": [
    "### Load the Concept Database model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J9Ft8PbFk7hc",
    "outputId": "76fd01ee-6914-4aca-e319-a0256089b10d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The CDB was exported by an unknown version of MedCAT.\n"
     ]
    }
   ],
   "source": [
    "cdb = CDB.load(DATA_DIR + \"/cdb.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xqmmAue-UE4"
   },
   "source": [
    "## Setting the CDB configuration\n",
    "\n",
    "The CDB config sets the model parameters.\n",
    "This allows you to tailor the model to your own specific use case. Although the default configuration will suit the majority of use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "eH1xiXvG-aWR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m        Config\n",
      "\u001b[1;31mString form:\u001b[0m\n",
      "{\n",
      "  \"annotation_output\": {\n",
      "    \"context_left\": -1,\n",
      "    \"context_right\": -1,\n",
      "    \"doc_extended_info\": false,\n",
      "    \"include_text_in_output\": false,\n",
      "    \"lowercase_context\": true\n",
      "  },\n",
      "  \"cdb_maker\": {\n",
      "    \"min_letters_required\": 2,\n",
      "    \"multi_separator\": \"|\",\n",
      "    \"name_versions\": [\n",
      "      \"LOWER\",\n",
      "      \"CLEAN\"\n",
      "    ],\n",
      "    \"remove_parenthesis\": 5\n",
      "  },\n",
      "  \"general\": {\n",
      "    \"checkpoint\": {\n",
      "      \"max_to_keep\": 1,\n",
      "      \"output_dir\": \"checkpoints\",\n",
      "      \"steps\": null\n",
      "    },\n",
      "    \"diacritics\": false,\n",
      "    \"full_unlink\": false,\n",
      "    \"log_format\": \"%(levelname)s:%(name)s: %(message)s\",\n",
      "    \"log_level\": 20,\n",
      "    \"log_path\": \"./medcat.log\",\n",
      "    \"make_pretty_labels\": null,\n",
      "    \"map_cui_to_group\": false,\n",
      "    \"separator\": \"~\",\n",
      "    \"show_nested_entities\": false,\n",
      "    \"spacy_disabled_components\": [\n",
      "      \"ner\",\n",
      "      \"parser\",\n",
      "      \"vectors\",\n",
      "      \"textcat\",\n",
      "      \"entity_linker\",\n",
      "      \"sentencizer\",\n",
      "      \"entity_ruler\",\n",
      "      \"merge_noun_chunks\",\n",
      "      \"merge_entities\",\n",
      "      \"merge_subtokens\"\n",
      "    ],\n",
      "    \"spacy_model\": \"en_core_web_md\",\n",
      "    \"spell_check\": true,\n",
      "    \"spell_check_deep\": false,\n",
      "    \"spell_check_len_limit\": 7,\n",
      "    \"workers\": 7\n",
      "  },\n",
      "  \"linking\": {\n",
      "    \"always_calculate_similarity\": false,\n",
      "    \"calculate_dynamic_threshold\": false,\n",
      "    \"context_ignore_center_tokens\": false,\n",
      "    \"context_vector_sizes\": {\n",
      "      \"long\": 18,\n",
      "      \"medium\": 9,\n",
      "      \"short\": 3,\n",
      "      \"xlong\": 27\n",
      "    },\n",
      "    \"context_vector_weights\": {\n",
      "      \"long\": 0.4,\n",
      "      \"medium\": 0.4,\n",
      "      \"short\": 0.1,\n",
      "      \"xlong\": 0.1\n",
      "    },\n",
      "    \"devalue_linked_concepts\": false,\n",
      "    \"disamb_length_limit\": 3,\n",
      "    \"filter_before_disamb\": false,\n",
      "    \"filters\": {\n",
      "      \"cuis\": {\n",
      "        \"py/set\": []\n",
      "      }\n",
      "    },\n",
      "    \"negative_ignore_punct_and_num\": true,\n",
      "    \"negative_probability\": 0.5,\n",
      "    \"optim\": {\n",
      "      \"base_lr\": 1,\n",
      "      \"min_lr\": 5e-05,\n",
      "      \"type\": \"linear\"\n",
      "    },\n",
      "    \"prefer_frequent_concepts\": 0.35,\n",
      "    \"prefer_primary_name\": 0.35,\n",
      "    \"random_replacement_unsupervised\": 0.8,\n",
      "    \"similarity_threshold\": 0.25,\n",
      "    \"similarity_threshold_type\": \"static\",\n",
      "    \"subsample_after\": 30000,\n",
      "    \"train\": true,\n",
      "    \"train_count_threshold\": 1,\n",
      "    \"weighted_average_function\": {\n",
      "      \"py/reduce\": [\n",
      "        {\n",
      "          \"py/type\": \"functools.partial\"\n",
      "        },\n",
      "        {\n",
      "          \"py/tuple\": [\n",
      "            {\n",
      "              \"py/function\": \"medcat.config.weighted_average\"\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"py/tuple\": [\n",
      "            {\n",
      "              \"py/function\": \"medcat.config.weighted_average\"\n",
      "            },\n",
      "            {\n",
      "              \"py/tuple\": []\n",
      "            },\n",
      "            {\n",
      "              \"factor\": 0.0004\n",
      "            },\n",
      "            {}\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"ner\": {\n",
      "    \"check_upper_case_names\": false,\n",
      "    \"max_skip_tokens\": 2,\n",
      "    \"min_name_len\": 3,\n",
      "    \"try_reverse_word_order\": false,\n",
      "    \"upper_case_limit_len\": 4\n",
      "  },\n",
      "  \"preprocessing\": {\n",
      "    \"do_not_normalize\": {\n",
      "      \"py/set\": [\n",
      "        \"JJS\",\n",
      "        \"VBD\",\n",
      "        \"VBG\",\n",
      "        \"JJR\",\n",
      "        \"VBN\",\n",
      "        \"VBP\"\n",
      "      ]\n",
      "    },\n",
      "    \"keep_punct\": {\n",
      "      \"py/set\": [\n",
      "        \":\",\n",
      "        \".\"\n",
      "      ]\n",
      "    },\n",
      "    \"max_document_length\": 1000000,\n",
      "    \"min_len_normalize\": 5,\n",
      "    \"skip_stopwords\": false,\n",
      "    \"stopwords\": null,\n",
      "    \"words_to_skip\": {\n",
      "      \"py/set\": [\n",
      "        \"nos\"\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"punct_checker\": {\n",
      "    \"pattern\": \"[^a-z0-9]+\",\n",
      "    \"py/object\": \"re.Pattern\"\n",
      "  },\n",
      "  \"version\": {\n",
      "    \"cdb_info\": {},\n",
      "    \"description\": \"No description\",\n",
      "    \"history\": [],\n",
      "    \"id\": null,\n",
      "    \"last_modified\": null,\n",
      "    \"location\": null,\n",
      "    \"medcat_version\": null,\n",
      "    \"meta_cats\": {},\n",
      "    \"ontology\": null,\n",
      "    \"performance\": {\n",
      "      \"meta\": {},\n",
      "      \"ner\": {}\n",
      "    }\n",
      "  },\n",
      "  \"word_skipper\": {\n",
      "    \"pattern\": \"^(nos)$\",\n",
      "    \"py/object\": \"re.Pattern\"\n",
      "  }\n",
      "}\n",
      "\u001b[1;31mFile:\u001b[0m        c:\\users\\aizaa\\anaconda3\\envs\\medcat\\lib\\site-packages\\medcat\\config.py\n",
      "\u001b[1;31mSource:\u001b[0m     \n",
      "\u001b[1;32mclass\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConfigMixin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;34m'id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# Will be: hash of most things \u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;34m'last_modified'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# Yep\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;34m'location'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# Path/URL/Whatever to where is this CDB located\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;34m'history'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# Populated automatically\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;34m'description'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"No description\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# General description and what it was trained on\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;34m'meta_cats'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# Populated automatically\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;34m'cdb_info'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# Populated automatically, output from cdb.print_stats\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;34m'performance'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'ner'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'meta'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# NER general performance, meta should be: {'meta': {'model_name': {'f1': <>, 'p': <>, ...}, ...}}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;34m'ontology'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# What was used to build the CDB, e.g. SNOMED_202009\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;34m'medcat_version'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# Which version of medcat was used to build the CDB\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# CDB Maker\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdb_maker\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# If multiple names or type_ids for a concept present in one row of a CSV, they are separted\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# by the character below.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'multi_separator'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'|'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Name versions to be generated.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'name_versions'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'LOWER'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'CLEAN'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Should preferred names with parenthesis be cleaned 0 means no, else it means if longer than or equal\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# e.g. Head (Body part) -> Head\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'remove_parenthesis'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Minimum number of letters required in a name to be accepted for a concept\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'min_letters_required'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# Used mainly to configure the output of the get_entities function, and in that also the output of\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m#get_json and multiprocessing\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotation_output\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'doc_extended_info'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'context_left'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'context_right'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'lowercase_context'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'include_text_in_output'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneral\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Logging config for everything | 'tagger' can be disabled, but will cause a drop in performance\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'log_level'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'log_format'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'%(levelname)s:%(name)s: %(message)s'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'log_path'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'./medcat.log'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'spacy_disabled_components'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ner'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'parser'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vectors'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'textcat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                                              \u001b[1;34m'entity_linker'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sentencizer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'entity_ruler'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'merge_noun_chunks'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                                              \u001b[1;34m'merge_entities'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'merge_subtokens'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# What model will be used for tokenization\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'spacy_model'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'en_core_web_md'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Separator that will be used to merge tokens of a name. Once a CDB is built this should\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m#always stay the same.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'separator'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'~'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Should we check spelling - note that this makes things much slower, use only if necessary. The only thing necessary\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m#for the spell checker to work is vocab.dat and cdb.dat built with concepts in the respective language.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'spell_check'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Should we process diacritics - for languages other than English, symbols such as 'é, ë, ö' can be relevant.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Note that this makes spell_check slower.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'diacritics'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# If True the spell checker will try harder to find mistakes, this can slow down\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m#things drastically.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'spell_check_deep'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Spelling will not be checked for words with length less than this\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'spell_check_len_limit'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# If set to True functions like get_entities and get_json will return nested_entities and overlaps\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'show_nested_entities'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# When unlinking a name from a concept should we do full_unlink (means unlink a name from all concepts, not just the one in question)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'full_unlink'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Number of workers used by a parallelizable pipeline component\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'workers'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Should the labels of entities (shown in displacy) be pretty or just 'concept'. Slows down the annotation pipeline\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m#should not be used when annotating millions of documents. If `None` it will be the string \"concept\", if `short` it will be CUI,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m#if `long` it will be CUI | Name | Confidence\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'make_pretty_labels'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# If the cdb.addl_info['cui2group'] is provided and this option enabled, each CUI will be maped to the group\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'map_cui_to_group'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Checkpointing config\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'checkpoint'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;31m# When doing training this is the name of the directory where checkpoints will be saved\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;34m'output_dir'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'checkpoints'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;31m# When training how often to save the checkpoint (one step represents one document), if None no ckpts will be created\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;34m'steps'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;31m# When training the maximum checkpoints will be kept on the disk\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;34m\"max_to_keep\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Should stopwords be skipped/ingored when processing input\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'skip_stopwords'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# This words will be completly ignored from concepts and from the text (must be a Set)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'words_to_skip'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nos'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# All punct will be skipped by default, here you can set what will be kept\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'keep_punct'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m':'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Nothing below this length will ever be normalized (input tokens or concept names), normalized means lemmatized in this case\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'min_len_normalize'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# If None the default set of stowords from spacy will be used. This must be a Set.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Documents longer  than this will be trimmed\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'max_document_length'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1000000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Should specific word types be normalized: e.g. running -> run\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'do_not_normalize'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'VBD'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VBG'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VBN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VBP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'JJS'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'JJR'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mner\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Do not detect names below this limit, skip them\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'min_name_len'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# When checkng tokens for concepts you can have skipped tokens inbetween\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m#used ones (usually spaces, new lines etc). This number tells you how many skipped can you have.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'max_skip_tokens'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Check uppercase to distinguish uppercase and lowercase words that have a different meaning.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'check_upper_case_names'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Any name shorter than this must be uppercase in the text to be considered. If it is not uppercase\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m#it will be skipped.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'upper_case_limit_len'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Try reverse word order for short concepts (2 words max), e.g. heart disease -> disease heart\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'try_reverse_word_order'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinking\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Should it train or not, this is set automatically ignore in 99% of cases and do not set manually\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Linear anneal\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'optim'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'linear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'base_lr'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'min_lr'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.00005\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# If <1 during unsupervised training the detected term will be randomly replaced with a probability of 1 - random_replacement_unsupervised\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m#Replaced with a synonym used for that term\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'random_replacement_unsupervised'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.80\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# 'optim': {'type': 'standard', 'lr': 1},\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# 'optim': {'type': 'moving_avg', 'alpha': 0.99, 'e': 1e-4, 'size': 100},\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# All concepts below this will always be disambiguated\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'disamb_length_limit'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Context vector sizes that will be calculated and used for linking\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'context_vector_sizes'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'xlong'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m27\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'long'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m18\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'medium'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'short'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Weight of each vector in the similarity score - make trainable at some point. Should add up to 1.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'context_vector_weights'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'xlong'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'long'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'medium'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'short'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# If True it will filter before doing disamb. Useful for the trainer.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'filter_before_disamb'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Concepts that have seen less training examples than this will not be used for\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m#similarity calculation and will have a similarity of -1.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'train_count_threshold'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Do we want to calculate context similarity even for concepts that are not ambigous.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'always_calculate_similarity'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Weights for a weighted average\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m#'weighted_average_function': partial(weighted_average, factor=0.02),\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'weighted_average_function'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweighted_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0004\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Concepts below this similarity will be ignored. Type can be static/dynamic - if dynamic each CUI has a different TH\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m#and it is calcualted as the average confidence for that CUI * similarity_threshold. Take care that dynamic works only\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m#if the cdb was trained with calculate_dynamic_threshold = True.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'calculate_dynamic_threshold'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'similarity_threshold_type'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'static'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'similarity_threshold'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Probability for the negative context to be added for each positive addition\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'negative_probability'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Do we ignore punct/num when negative sampling\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'negative_ignore_punct_and_num'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# If >0 concepts for which a detection is its primary name will be preferred by that amount (0 to 1)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'prefer_primary_name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.35\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# If >0 concepts that are more frequent will be prefered by a multiply of this amount\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'prefer_frequent_concepts'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.35\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# DISABLED in code permanetly: Subsample during unsupervised training if a concept has received more than\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'subsample_after'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m30000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# When adding a positive example, should it also be treated as Negative for concepts\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m#which link to the postive one via names (ambigous names).\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'devalue_linked_concepts'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# If true when the context of a concept is calculated (embedding) the words making that concept are not taken into accout\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'context_ignore_center_tokens'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;31m# Filters\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;34m'filters'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;34m'cuis'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# CUIs in this filter will be included, everything else excluded, must be a set, if empty all cuis will be included\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;33m}\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# Some regex that we will need\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_skipper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'^({})$'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'|'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words_to_skip'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# Very agressive punct checker, input will be lowercased\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunct_checker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[^a-z0-9]+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;31m# Override\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mrebuild_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# Some regex that we will need\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_skipper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'^({})$'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'|'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words_to_skip'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# Very agressive punct checker, input will be lowercased\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunct_checker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[^a-z0-9]+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mget_hash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mhasher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHasher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'version'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'general'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'linking'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mhasher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32melif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'general'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;32mfor\u001b[0m \u001b[0mk2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[0mk2\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'spacy_model'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                        \u001b[0mhasher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                        \u001b[1;31m# Ignore spacy model\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                        \u001b[1;32mpass\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32melif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'linking'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;32mfor\u001b[0m \u001b[0mk2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[0mk2\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"filters\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                        \u001b[0mhasher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                        \u001b[0mhasher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mhasher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "# For further information on the cdb configuration options and explore what the default options are.\n",
    "??cdb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "-aLoW6ltAa6l"
   },
   "outputs": [],
   "source": [
    "# Set a couple of parameters, they are usually set via environments, but\n",
    "#here we will do it explicitly. You can read more about each option in the \n",
    "#medcat repository: https://github.com/CogStack/MedCAT\n",
    "\n",
    "cdb.config.ner['min_name_len'] = 2\n",
    "cdb.config.ner['upper_case_limit_len'] = 3\n",
    "cdb.config.general['spell_check'] = True\n",
    "cdb.config.linking['train_count_threshold'] = 10\n",
    "cdb.config.linking['similarity_threshold'] = 0.3\n",
    "cdb.config.linking['train'] = True\n",
    "cdb.config.linking['disamb_length_limit'] = 5\n",
    "cdb.config.general['full_unlink'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqmVITvWCIr6"
   },
   "source": [
    "Note: Don't forget to save the cdb with the new configurations if you want to reuse them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZvhmkIL8433"
   },
   "source": [
    "# Create a MedCAT model pack\n",
    "\n",
    "A MedCAT model pack is an easy way to store all the various components of a MedCAT model in one place.\n",
    "\n",
    "This includes the CDB, CDB configurations, Vocab, and even various MetaCAT models! We will learn more about the latter in the following tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "LxcW2GcM-c-M"
   },
   "outputs": [],
   "source": [
    "# Initialise the model\n",
    "cat = CAT(cdb=cdb, config=cdb.config, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 780
    },
    "id": "-cXusFuI-drw",
    "outputId": "9cbe9bfd-27cb-458e-b967-b466b4cac6e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:medcat:Please consider populating the version information [description, performance, location, ontology] in cat.config.version\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      " avg = a.mean(axis)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      " ret = ret.dtype.type(ret / rcount)\n",
      "WARNING:medcat:Please consider updating [description, performance, location, ontology] in cat.config.version\n",
      "WARNING:medcat:This will save all models into a zip file, can take some time and require quite a bit of disk space.\n",
      "INFO:medcat:{\n",
      " \"Model ID\": \"a84c91ebfeb4f727\",\n",
      " \"Last Modified On\": \"25 August 2022\",\n",
      " \"History (from least to most recent)\": [],\n",
      " \"Description\": \"No description\",\n",
      " \"Source Ontology\": null,\n",
      " \"Location\": null,\n",
      " \"MetaCAT models\": [],\n",
      " \"Basic CDB Stats\": {\n",
      " \"Number of concepts\": 2,\n",
      " \"Number of names\": 7,\n",
      " \"Number of concepts that received training\": 0,\n",
      " \"Number of seen training examples in total\": 0,\n",
      " \"Average training examples per concept\": NaN\n",
      " },\n",
      " \"Performance\": {\n",
      " \"ner\": {},\n",
      " \"meta\": {}\n",
      " },\n",
      " \"Important Parameters (Partial view, all available in cat.config)\": {\n",
      " \"config.ner['min_name_len']\": {\n",
      " \"value\": 2,\n",
      " \"description\": \"Minimum detection length (found terms/mentions shorter than this will not be detected).\"\n",
      " },\n",
      " \"config.ner['upper_case_limit_len']\": {\n",
      " \"value\": 3,\n",
      " \"description\": \"All detected terms shorter than this value have to be uppercase, otherwise they will be ignored.\"\n",
      " },\n",
      " \"config.linking['similarity_threshold']\": {\n",
      " \"value\": 0.3,\n",
      " \"description\": \"If the confidence of the model is lower than this a detection will be ignore.\"\n",
      " },\n",
      " \"config.general['spell_check']\": {\n",
      " \"value\": true,\n",
      " \"description\": \"Is spell checking enabled.\"\n",
      " },\n",
      " \"config.general['spell_check_len_limit']\": {\n",
      " \"value\": 7,\n",
      " \"description\": \"Words shorter than this will not be spell checked.\"\n",
      " }\n",
      " },\n",
      " \"MedCAT Version\": \"1.3.0\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'medcat_model_pack_a84c91ebfeb4f727'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and save a model pack\n",
    "cat.create_model_pack(DATA_DIR + \"my_first_medcat_modelpack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fwiKys4k7he"
   },
   "source": [
    "# End\n",
    "\n",
    "This is everything you need to create your own MedCAT models. In the following tutorials you will learn how to uses modelpacks to train models and use them to annotate documents. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MedCAT Tutorial | Part 3.1 Building a Concept Database and Vocabulary.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "medcat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "390e5a415a5fdbdbe7be3f24d612e01396b685875a8fec625d9a2a4fa6344806"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
